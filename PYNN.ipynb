{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pydevcasts/Dr.RahimPour_ml_practise/blob/main/PYNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "کد ارائه‌شده یک پیاده‌سازی ساده از شبکه عصبی با استفاده از الگوریتم پس‌انتشار (Back-Propagation) در زبان برنامه‌نویسی پایتون است. این شبکه برای یادگیری تابع XOR طراحی شده است. در ادامه، اجزای اصلی کد و عملکرد آن به‌طور خلاصه توضیح داده می‌شود:\n",
        "\n",
        "### اجزای اصلی کد:\n",
        "\n",
        "1. **وارد کردن کتابخانه‌ها**:\n",
        "   - `math`: برای استفاده از توابع ریاضی.\n",
        "   - `random`: برای تولید اعداد تصادفی.\n",
        "\n",
        "2. **توابع کمکی**:\n",
        "   - `rand(a, b)`: یک عدد تصادفی بین `a` و `b` تولید می‌کند.\n",
        "   - `makeMatrix(I, J, fill=0.0)`: یک ماتریس دو بعدی با ابعاد `I` در `J` ایجاد می‌کند و تمام عناصر آن را با مقدار `fill` پر می‌کند.\n",
        "   - `sigmoid(x)`: تابع سیگموئید (tanh) را محاسبه می‌کند.\n",
        "   - `dsigmoid(y)`: مشتق تابع سیگموئید را محاسبه می‌کند.\n",
        "\n",
        "3. **کلاس NN**:\n",
        "   - `__init__(self, ni, nh, no)`: سازنده کلاس که تعداد نودهای ورودی (`ni`)، نودهای مخفی (`nh`) و نودهای خروجی (`no`) را تعریف می‌کند. همچنین وزن‌ها را به صورت تصادفی مقداردهی می‌کند.\n",
        "   - `update(self, inputs)`: ورودی‌ها را به شبکه می‌دهد و فعالیت‌های نودهای مخفی و خروجی را محاسبه می‌کند.\n",
        "   - `backPropagate(self, targets, N, M)`: خطا را محاسبه کرده و وزن‌ها را به‌روز می‌کند. `N` نرخ یادگیری و `M` عامل مومنتوم است.\n",
        "   - `test(self, patterns)`: شبکه را با الگوهای ورودی آزمایش می‌کند.\n",
        "   - `weights(self)`: وزن‌های ورودی و خروجی را چاپ می‌کند.\n",
        "   - `train(self, patterns, iterations=1000, N=0.5, M=0.1)`: شبکه را با استفاده از الگوهای ورودی آموزش می‌دهد.\n",
        "\n",
        "4. **تابع demo()**:\n",
        "   - الگوهای ورودی و خروجی برای تابع XOR را تعریف می‌کند.\n",
        "   - یک شبکه عصبی با دو نود ورودی، دو نود مخفی و یک نود خروجی ایجاد می‌کند.\n",
        "   - شبکه را با الگوهای تعریف‌شده آموزش می‌دهد و سپس نتایج را آزمایش می‌کند.\n",
        "\n",
        "### نتیجه‌گیری:\n",
        "\n",
        "این کد یک شبکه عصبی ساده را پیاده‌سازی می‌کند که می‌تواند تابع \n",
        "XOR \n",
        "را یاد بگیرد. با استفاده از الگوریتم پس‌انتشار، شبکه وزن‌های خود را به‌روز می‌کند تا خطای پیش‌بینی را کاهش دهد.\n",
        " این پیاده‌سازی می‌تواند به عنوان یک نقطه شروع برای درک عمیق‌تر شبکه‌های عصبی و یادگیری ماشین استفاده شود."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RFxw-cmLqex"
      },
      "source": [
        "https://colab.research.google.com/drive/1_-Ez1p73aOEP629zJJHIl4iReIHqqNWr\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4GX9GWWKtXE",
        "outputId": "2fef2db9-ca44-4fa8-9118-77eade4979fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 0] -> [0.00424108155062589]\n",
            "[0, 1] -> [0.9821508029410748]\n",
            "[1, 0] -> [0.9820129388618121]\n",
            "[1, 1] -> [-0.0011469114721422528]\n"
          ]
        }
      ],
      "source": [
        "# Back-Propagation Neural Networks\n",
        "#\n",
        "# Written in Python.  See http://www.python.org/\n",
        "# Placed in the public domain.\n",
        "# Neil Schemenauer <nas@arctrix.com>\n",
        "\n",
        "import math\n",
        "import random\n",
        "import string\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "# calculate a random number where:  a <= rand < b\n",
        "def rand(a, b):\n",
        "    return (b-a)*random.random() + a\n",
        "\n",
        "# Make a matrix (we could use NumPy to speed this up)\n",
        "def makeMatrix(I, J, fill=0.0):\n",
        "    m = []\n",
        "    for i in range(I):\n",
        "        m.append([fill]*J)\n",
        "    return m\n",
        "\n",
        "# our sigmoid function, tanh is a little nicer than the standard 1/(1+e^-x)\n",
        "def sigmoid(x):\n",
        "    return math.tanh(x)\n",
        "\n",
        "# derivative of our sigmoid function, in terms of the output (i.e. y)\n",
        "def dsigmoid(y):\n",
        "    return 1.0 - y**2\n",
        "\n",
        "class NN:\n",
        "    def __init__(self, ni, nh, no):\n",
        "        # number of input, hidden, and output nodes\n",
        "        self.ni = ni + 1 # +1 for bias node\n",
        "        self.nh = nh\n",
        "        self.no = no\n",
        "\n",
        "        # activations for nodes\n",
        "        self.ai = [1.0]*self.ni\n",
        "        self.ah = [1.0]*self.nh\n",
        "        self.ao = [1.0]*self.no\n",
        "\n",
        "        # create weights\n",
        "        self.wi = makeMatrix(self.ni, self.nh)\n",
        "        self.wo = makeMatrix(self.nh, self.no)\n",
        "        # set them to random vaules\n",
        "        for i in range(self.ni):\n",
        "            for j in range(self.nh):\n",
        "                self.wi[i][j] = rand(-0.2, 0.2)\n",
        "        for j in range(self.nh):\n",
        "            for k in range(self.no):\n",
        "                self.wo[j][k] = rand(-2.0, 2.0)\n",
        "\n",
        "        # last change in weights for momentum\n",
        "        self.ci = makeMatrix(self.ni, self.nh)\n",
        "        self.co = makeMatrix(self.nh, self.no)\n",
        "\n",
        "    def update(self, inputs):\n",
        "        if len(inputs) != self.ni-1:\n",
        "            raise ValueError('wrong number of inputs')\n",
        "\n",
        "        # input activations\n",
        "        for i in range(self.ni-1):\n",
        "            #self.ai[i] = sigmoid(inputs[i])\n",
        "            self.ai[i] = inputs[i]\n",
        "\n",
        "        # hidden activations\n",
        "        for j in range(self.nh):\n",
        "            summ = 0.0\n",
        "            for i in range(self.ni):\n",
        "                summ = summ + self.ai[i] * self.wi[i][j]\n",
        "            self.ah[j] = sigmoid(summ)\n",
        "\n",
        "        # output activations\n",
        "        for k in range(self.no):\n",
        "            summ = 0.0\n",
        "            for j in range(self.nh):\n",
        "                summ = summ + self.ah[j] * self.wo[j][k]\n",
        "            self.ao[k] = sigmoid(summ)\n",
        "\n",
        "        return self.ao[:]\n",
        "\n",
        "\n",
        "    def backPropagate(self, targets, N, M):\n",
        "        if len(targets) != self.no:\n",
        "            raise ValueError('wrong number of target values')\n",
        "\n",
        "        # calculate error terms for output\n",
        "        output_deltas = [0.0] * self.no\n",
        "        for k in range(self.no):\n",
        "            error = targets[k]-self.ao[k]\n",
        "            output_deltas[k] = dsigmoid(self.ao[k]) * error\n",
        "\n",
        "        # calculate error terms for hidden\n",
        "        hidden_deltas = [0.0] * self.nh\n",
        "        for j in range(self.nh):\n",
        "            error = 0.0\n",
        "            for k in range(self.no):\n",
        "                error = error + output_deltas[k]*self.wo[j][k]\n",
        "            hidden_deltas[j] = dsigmoid(self.ah[j]) * error\n",
        "\n",
        "        # update output weights\n",
        "        for j in range(self.nh):\n",
        "            for k in range(self.no):\n",
        "                change = output_deltas[k]*self.ah[j]\n",
        "                self.wo[j][k] = self.wo[j][k] + N*change + M*self.co[j][k]\n",
        "                self.co[j][k] = change\n",
        "                #print N*change, M*self.co[j][k]\n",
        "\n",
        "        # update input weights\n",
        "        for i in range(self.ni):\n",
        "            for j in range(self.nh):\n",
        "                change = hidden_deltas[j]*self.ai[i]\n",
        "                self.wi[i][j] = self.wi[i][j] + N*change + M*self.ci[i][j]\n",
        "                self.ci[i][j] = change\n",
        "\n",
        "        # calculate error\n",
        "        error = 0.0\n",
        "        for k in range(len(targets)):\n",
        "            error = error + 0.5*(targets[k]-self.ao[k])**2\n",
        "        return error\n",
        "\n",
        "\n",
        "    def test(self, patterns):\n",
        "        for p in patterns:\n",
        "            print(p[0], '->', self.update(p[0]))\n",
        "\n",
        "    def weights(self):\n",
        "        print ('Input weights:')\n",
        "        for i in range(self.ni):\n",
        "            print (self.wi[i])\n",
        "        print\n",
        "        print('Output weights:')\n",
        "        for j in range(self.nh):\n",
        "            print(self.wo[j])\n",
        "\n",
        "    def train(self, patterns, iterations=1000, N=0.5, M=0.1):\n",
        "        # N: learning rate\n",
        "        # M: momentum factor\n",
        "        for i in range(iterations):\n",
        "            error = 0.0\n",
        "            for p in patterns:\n",
        "                inputs = p[0]\n",
        "                targets = p[1]\n",
        "                self.update(inputs)\n",
        "                error = error + self.backPropagate(targets, N, M)\n",
        "            if i % 100 == 0:\n",
        "                pass #print 'error %-14f' % error\n",
        "\n",
        "\n",
        "def demo():\n",
        "    # Teach network XOR function\n",
        "    pat = [\n",
        "        [[0,0], [0]],\n",
        "        [[0,1], [1]],\n",
        "        [[1,0], [1]],\n",
        "        [[1,1], [0]]\n",
        "    ]\n",
        "\n",
        "    # create a network with two input, two hidden, and one output nodes\n",
        "    n = NN(2, 2, 1)\n",
        "    # train it with some patterns\n",
        "    n.train(pat)\n",
        "    # test it\n",
        "    n.test(pat)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    demo()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMcN4cLsR4GnjrBwrIN9ufv",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
